---
title: "Collection (La Junta)"
author: "Connor Krenzer"
date: "3/11/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup-packages, include = FALSE, eval = TRUE}

knitr::opts_chunk$set(echo = TRUE,
                      include = TRUE,
                      results = "hide",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)

# Packages
if(!require(pacman)) install.packages("pacman")
pacman::p_load(rvest, stringr, tidyr, readr, dplyr, lubridate, clock, ggplot2, htmltools, htmlwidgets)

# The collection() function
source("Collection/functions/collection().R")

```


# Implementation

I have been changing the URLs around to find different data, and the market reports seem to start around ID == 6700. The max URL at the time of writing is around 14000. The data begins in 2016.

```{r urls-execute, eval = TRUE}

# This parses together different market report IDs and stores them in a vector
# (useful for when you do not know the URLs beforehand)
#urls <- paste0("http://www.winterlivestock.com/lajunta.php?reportID=", 6680:14000, "#marketreport")


#the most recent market report, without the permalink
#NOTE: the URL column will not contain the market report number!!!
#urls <- "http://www.winterlivestock.com/lajunta.php"


# This contains all the La Junta market report URLs for easy access
# (recommended if you plan to re-collect the data from scratch--just
# do not forget the possibility that some market reports could have
# been missed, if you really think it necessary to recollect this data)
# just be sure to set `prevent_use_of_previous_urls` to FALSE
urls <- read_lines("Collection/La Junta URLs.txt")
urls <- read_lines("https://raw.githubusercontent.com/Ckrenzer/Winter-Livestock-Data/main/Collection/La%20Junta%20URLs.txt")
#collection(urls = urls, prevent_use_of_previous_urls = FALSE)


# the last-used market report URL
urls <- "http://www.winterlivestock.com/lajunta.php?reportID=13741#marketreport"


# This line is used to automate the update process through RPAs
urls <- read_lines("Collection/current_url_Lajunta.txt")

# No need to save the output--this function writes to a csv file...
collection(urls = urls)

```






# Remove File

If you would like to remove the csv file on your computer when updating data (meaning you would like to start from scratch), you can run the below chunk to remove the file.

```{r remove csv, eval = FALSE}

if(file.exists("La Junta Market Reports.csv")){
  file.remove("La Junta Market Reports.csv")
}

```





# Testing


Of course, we want to make sure everything looks good, no? This section identifies problem areas in the file.

```{r testing, eval = FALSE}
#lajunta <- read_csv("https://raw.githubusercontent.com/Ckrenzer/Winter-Livestock-Data/main/La%20Junta%20Market%20Reports.csv")

lajunta <- read_csv("La Junta Market Reports.csv")



# finding market reports using the most recent market report instead of the permalink.
lajunta %>% 
  filter(URL == "http://www.winterlivestock.com/lajunta.php") %>% 
  as.data.frame()




# finds counts of all types--good for finding how many types are in the data
count(lajunta, Type) %>% 
  arrange(desc(n)) %>% 
  as.data.frame()

# finds counts of all reproductive statuses--there should be five: bull, cow, steer, heifer, and NA (if there are missing values).
count(lajunta, Reprod) %>% 
  arrange(desc(n)) %>% 
  as.data.frame()

# How many sales were there on a given day?
lajunta %>% 
  count(Date)

# Taking a look at the newest data
tail(lajunta)


# MISSING VALUES _-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-

# relevant info when imputing values:
lajunta %>% 
  group_by(Reprod) %>% 
  dplyr::summarize("Median price" = median(Price, na.rm = T),
                   "Median weight" = median(Weight, na.rm = T),
                   "Median quantity" = median(Quantity, na.rm = T),
                   "Mean Price" = mean(Price, na.rm = T),
                   "Mean weight" = mean(Weight, na.rm = T),
                   "Mean quantity" = mean(Quantity))
# KNN is probably the best imputation method for this data



# DATE -__-__-__-__-__-__-__-__-__-__-
# how many missing values are in the Date column?
lajunta %>% 
  filter(is.na(Date)) %>% 
  slice_sample(n = 20)


# BUYER _--_--_--_--_--_--_--_--_--_--_
lajunta %>% 
  filter(is.na(Buyer))



# QUANTITY -__-__-__-__-__-__-__-__-__-__-
lajunta %>% 
  filter(is.na(Quantity))



# TYPE _--_--_--_--_--_--_--_--_--_--_
lajunta %>% 
  filter(is.na(Type))



# WEIGHT -__-__-__-__-__-__-__-__-__-__-
lajunta %>% 
  filter(is.na(Weight))



# PRICE _--_--_--_--_--_--_--_--_--_--_
lajunta %>% 
  filter(is.na(Price))



# REPROD -__-__-__-__-__-__-__-__-__-__-
# see any patterns?
lajunta %>% 
  dplyr::select(Type, Weight, Reprod, URL) %>% 
  filter(is.na(Reprod)) %>% 
  as.data.frame()

# You can see the distribution of missing Reprod values
lajunta %>% 
  filter(is.na(Reprod)) %>% 
  ggplot() +
  geom_col(mapping = aes(x = Weight, y = Price))

```
