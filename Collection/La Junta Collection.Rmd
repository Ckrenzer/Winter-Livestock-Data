---
title: "Collection (La Junta)"
author: "Connor Krenzer"
date: "3/11/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Introduction

I figure it's time to dig a bit deeper into Winter Livestock's market reports. I plan on checking all the market reports on the website to find historical data--but only for La Junta, CO. The old method works well enough, but limitations in available data using the old scripts make this approach far more lucrative.

This implementation assumes a few things about the structure of the text, so there may be small issues here and there with the data.

Some improvements that could be made in the future are the following:

-   The collection() function currently writes the pre-cleaned data to the cleaned 'La Junta Market Reports.csv' if 'La Junta Market Reports.csv' is already on your computer. One solution is to append the uncleaned data to the (before cleaning) csv file and then reclean the entire (before cleaning) file? Other solutions are possible. One that seems more efficient is to remove 'La Junta Market Reports.csv', rename the (before cleaning) file to 'La Junta Market Reports.csv', then append the uncleaned data to that. Again, these are big picture ideas--just pick one that works and stick with it (update README, gitignore, and Data KEY as necessary).

-   Check the date to make sure that you aren't 'double-counting' the same market report that has two IDs (just be sure to handle NA dates carefully) before adding it to the csv. You may have to do this manually after finding the URL.

-   Find out why some dates are not found when pulling the data.

-   Store the urls into a text file so that time no longer has to be wasted searching for the correct url (just make the urls variable in the "urls-execute" section a vector of the contents of the file containing urls).

-   Store the urls in a named vector for easy debugging and identification of repeated values (the date should be the vector's name)

-   Look at NA dates and see if you can find out where things go awry (one problem I've found is that the year is sometimes not provided on multi-day sales).

-   Add the word 'internet' to a keyword removal list to skip over results from an internet auction?

-   Improve the models (particularly the linear models)

-   Simplify the cleaning script--it's a hot mess!

# How It Works

To collect the data, I will first check whether the webpage text requires "\n" or "\r" as its delimiter. How do you do that? I'm glad you asked. I will do a strsplit() on the text data with "\n" as the delimiter. If the length of the resulting list is less than 30, I will try again using "\r" as the delimiter. If neither work, the algorithm moves onto the next market report. Most market reports are empty.

Once we have a market report, we determine whether the market report is a La Junta market report by searching for 'la junta' in the text.

After that, we extract the date from the text, remove the heading, remove information that is not about a sale, extract the buyer name and parse it together with the rest of the sale info, save everything to a data frame, then write the data frame to a csv. SAVE A COPY OF THIS CSV FILE TO ANOTHER FOLDER SO THAT YOU DON'T HAVE TO RE-RUN THE collection() FUNCTION IF THE CLEANING CHUNK HAS PROBLEMS.

Once we have a csv file, we clean it. Basically, this means you become best friends with stringr and abuse regex until you can only understand your code via the comments.

Then we do some EDA to find any issues we missed. This is where that copy of the csv file comes in handy! If all that works out, the csv file is ready for use!

```{r setup-packages, include = FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      include = TRUE,
                      results = "hide",
                      error = F,
                      warning = F,
                      message = F)

# Packages
if(!require(pacman)) install.packages("pacman")
pacman::p_load(rvest, stringr, tidyr, readr, dplyr, lubridate, ggplot2)

```

# Remove File

If you would like to remove the csv file on your computer when updating data (meaning you would like to start from scratch), you can run the below chunk to remove the file.

```{r remove csv}

if(file.exists("La Junta Market Reports.csv")){
  file.remove("La Junta Market Reports.csv")
}

```

# Collection()

This function is a wrapper around the web scraper. Calls to str_view() are unnecessary and may even slow down the program by a negligible amount, but I'm keeping them in to simplify debugging.

```{r collection()}

collection <- function(urls){
  
  
  for(URL in urls){
    
    
    # simple yet effective way of showing the operation's progress
    cat(".")
    
    
    
    # Reading in data from webpage ------------------------------
    # Saving the webpage into a variable
    webpage <- read_html(URL)
    
    # Saving data from the webpage written in html
    # The nodes are consistent across market reports
    livestock_data_html <- html_nodes(webpage, "div:nth-child(9) div.sml")
    
    # Converting to plain text
    livestock_data <- html_text(livestock_data_html)
    
    
    
    
    
    
    
    
    # Determining appropriate delimiter -------------------------
    # The first delimiter we will try
    delimiter <- "\n"
    
    # Asks whether the list has more than 30 entries after the strsplit(). If there is only one entry, then that means the format uses "\r" as its delimiter or does not contain information we care about.
    if(length(strsplit(livestock_data, delimiter)[[1]]) < 30){
      
      # If the first delimiter does not get us what we are looking for, we will try the second delimiter instead.
      delimiter <- "\r" 
      
      # Now we test the second delimiter...
      if(length(strsplit(livestock_data, delimiter)[[1]]) < 30){
        
        # If the second delimiter fails to deliver, skip to the next iteration of the loop (this webpage does not contain information we care about).
        next 
        
      }#end of nested if()
    }#end of if()
    
    
    
    
    
    
    
    
    
    # Determining location (La Junta) -------------------------
    # If we've made it this far, that means that there is information about a market report on the webpage.
    # We now need to determine whether this market report is for La Junta, CO.
    # This shouldn't be too hard. We can assume that "La Junta" will not appear in non-La Junta, CO market reports. Conversely, if we find "la junta" anywhere in the text, then we found a La Junta market report.
    
    
    # making all the text lowercase (to avoid the "la junta" != "La Junta" problem)
    livestock_data <- str_to_lower(livestock_data)
    
    # Now we can search for "la junta" in the text
    if(!str_detect(livestock_data, pattern = "la junta")){
      
      # If "la junta" is not found in the data, that means that this is not a La Junta market report--skip to the next iteration of the loop.
      next
    }
    
    
    
    
    
    
    
    
    
    
    # Finding the date of sale ----------------------------------------
    # Let's store the results from the strsplit() into a variable to more easily extract the date
    livestock_data <- strsplit(livestock_data, delimiter)
    
    
    # Saves the first entry that finds digits followed by the word "cattle"--this entry contains the sentence from which we can pull the date
    date_of_sale <- livestock_data[[1]][sapply(livestock_data, str_detect, "\\d+\\s*cattle") %>% 
                                          which() %>% 
                                          min()]
    # removing punctuation from the sentence
    date_of_sale <- str_remove_all(date_of_sale, "\\.|,|;")
    
    
    # extracting the month the sale took place from the sentence
    month_of_sale <- str_extract(date_of_sale, "january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|jun|jul|aug|sept|oct|nov|dec")
    
    # the date of the sale in "month ##nd YYYY" format (Ex. "march 9th 2021")
    #note: some sales take place over two days, so we have to remove the "& ##th" following the first day's date
    date_of_sale <- str_extract(date_of_sale, paste0(month_of_sale, "\\s+\\d{1,2}[a-z0-9&\\s]{0,15}\\d{4}")) %>% 
      #replacing the month name with its corresponding number
      str_replace_all(month_of_sale,
                      case_when(month_of_sale == "january" ~ "1",
                                month_of_sale == "february" ~ "2",
                                month_of_sale == "march" ~ "3",
                                month_of_sale == "april" ~ "4",
                                month_of_sale == "may" ~ "5",
                                month_of_sale == "june" ~ "6",
                                month_of_sale == "july" ~ "7",
                                month_of_sale == "august" ~ "8",
                                month_of_sale == "september" ~ "9",
                                month_of_sale == "october" ~ "10",
                                month_of_sale == "november" ~ "11",
                                month_of_sale == "december" ~ "12",
                                month_of_sale == "jan" ~ "1",
                                month_of_sale == "feb" ~ "2",
                                month_of_sale == "mar" ~ "3",
                                month_of_sale == "apr" ~ "4", #you don't need to repeat MAY, remember
                                month_of_sale == "jun" ~ "6",
                                month_of_sale == "jul" ~ "7",
                                month_of_sale == "aug" ~ "8",
                                month_of_sale == "sept" ~ "9",
                                month_of_sale == "oct" ~ "10",
                                month_of_sale == "nov" ~ "11",
                                month_of_sale == "dec" ~ "12",
                      )) %>% 
      #removing letters and replacing space characters with hyphens
      str_remove_all("[a-z]") %>%
      str_replace_all("\\s+", " ") %>% #replacing multiple spaces with just one space
      str_replace_all("\\s", "-")
    
    
    
    
    
    
    
    
    
    
    
    
    # Dealing with multi-day sales ----------------------------
    
    # Checks if the sale took place over two days (the date wasn't provided if the value is null)
    if(!is.na(date_of_sale) & !str_detect(date_of_sale, "\\d{1,2}-\\d{1,2}-\\d{4}")){
      
      # If this is a two-day sale, then we keep only the first day
      date_of_sale <- str_replace_all(date_of_sale, "(\\d{1,2}-\\d{1,2})-&*-\\d{1,2}(-\\d{4})", "\\1\\2")
    }
    
    
    # Let's store everything back into a character vector
    livestock_data <- as.vector(livestock_data[[1]])
    
    
    
    
    
    
    
    
    
    
    
    
    # Header Removal with Keywords ------------------------------------------------------
    # A set of keywords designed to remove heading information
    # and other information we are not interested in observing
    keywords <- "\\s+sold|\\s+sale|\\s+monday|\\s+tuesday|\\s+wednesday|\\s+thursday|\\s+friday|\\s+saturday|\\s+sunday|\\s+receipts|\\s+through|\\s+mostly|\\s+winter|\\s+summer|\\s+spring|\\s+fall|\\s+autumn|\\s+is\\s+|\\s+next|\\s+quality|\\s+mostly|\\s+noon|\\s+early|\\s+stock|\\s+steady|\\s+test\\s+|\\s+offer|\\s+selection|\\s+week|\\s+annual|\\s+package|consigned|\\s*now\\s+|special\\s+|\\s+higher|calves\\s&\\syearlings\\s*$|\\s+am\\s+|\\s+pm\\s+|\\s+a.m.\\s+|\\s+p.m.\\s+|report[:]?\\s+|la\\s+junta,|\\s+co$|\\*$"
    
    
    # Removes headings and unrelated information from the data
    livestock_data <- livestock_data[!str_detect(livestock_data, keywords)]
    
    
    
    
    
    
    
    
    
    # Removing Unwanted Sections -------------------------------------------------------
    
    # We can pull out the sales information by removing lines we do not
    # care about. Since we know that the information we want is stored
    # in lines that are much shorter than the others, we can pull out
    # lines that have fewer characters than some optimal number.
    # I chose 60. In other words, I am keeping only those lines
    # (which are stored as elements in the vector) that contain
    # fewer than 60 characters.
    livestock_data <- livestock_data[-c(which((nchar(livestock_data) > 60)))]
    
    
    # The livestock data starts each day with a person's name and then has the quantity, type, weight, and price
    # if the person made more than one purchase, the line starts with "\n\t\t"--this is the reason
    # why we cannot make use of str_trim(). We need the "\n\t\t" to indicate whether
    # that element in the vector is really another purchase by the same buyer.
    
    # You can see what I am talking about by running
    # the code below. The "\n\t\t" is hidden, but
    # you can see the highlighted boxes at the
    # beginning of the lines containing the pattern:
    str_view(livestock_data, "\n\t\t")
    
    
    
    
    
    
    
    
    
    
    
    
    
    # Buyer names -----------------------------------------------------
    # Step 1. Names of Buyers:
    # Pulling out the indices that contain the buyer's name.
    buyers <- livestock_data[!str_detect(livestock_data, "\n\t\t")]
    
    # removing leading newline characters
    buyers <- str_trim(buyers)
    
    # Extracting the buyer's name--the name ends when
    # the first "\t" is encountered. You can see that
    # with this line:
    str_view(buyers, ".*?(?=\t)")
    
    # The names of the buyers:
    buyers <- str_extract(buyers, ".*?(?=\t)")
    
    # removing unneeded white space
    buyers <- str_trim(buyers)
    
    
    
    
    
    
    
    
    
    
    # Adding Buyer Names Back In ----------------------------------------------
    # This section uses ID numbers to identify the buyer for
    # each sale listed on the market report. It then adds
    # the buyer's name back onto the lines from which it
    # was omitted.
    
    # I will provide each buyer an ID number to determine
    # which purchases that buyer made. I am not crazy about
    # using loops, as I could probably write a function
    # to do this task for me, but it does everything we
    # need it to.
    current_ID <- 0
    for(i in 1:length(livestock_data)){
      if(str_detect(livestock_data[i], "\n\t\t")){
        # Only multiple purchases will have the "\n\t\t" so we can
        # use "\n\t\t" as a placeholder for the beginning of the string,
        # where the buyer's name goes, and make it look the same as the
        # lines where the name is present
        livestock_data[i] <- str_replace(livestock_data[i], "\n\t\t",  paste(buyers[current_ID], "\t", sep = ""))
        
      } else if(!str_detect(livestock_data[i], "\n\t\t")){
        # if the index lands on a new buyer, give them
        # the next ID number
        current_ID <- current_ID + 1 
      }
      
    }# end of for loop
    
    # removing leading and trailing white space
    livestock_data <- str_trim(livestock_data)
    
    
    
    
    
    
    
    
    
    # Data frame ---------------------------------------------------
    # Making a data frame
    livestock_data <- tibble(livestock_data)
    
    
    # making new columns based off the sections
    # separated by "\t"
    livestock_data <- livestock_data %>% 
      separate(livestock_data, into = c("buyer", "quantity", "weight", "price"), sep = "\t")
    
    # Replaces the space separating the quantity and the type with a semicolon
    livestock_data$quantity <- str_replace(livestock_data$quantity, " ", ";")
    
    # Making quantity and type their own columns
    livestock_data <- livestock_data %>% 
      separate(quantity, into = c("quantity", "type"), sep = ";")
    
    
    # Making quantity, weight, and price numeric datatypes
    livestock_data[c(2, 4, 5)] <- sapply(livestock_data[c(2, 4, 5)], as.numeric)
    
    # Adding the date from the market report as a column
    
    
    
    # We are just about there! All that remains is removing the section
    # with NA's introduced:
    as.data.frame(livestock_data)
    
    # How do we fix that? Well, since we may want that information
    # for future use, we won't want to remove it entirely with a
    # keyword search, as was done in the beginning. What we can
    # remove observations with NA values to keep only the data
    # that matches our desired format:
    livestock_data <- na.omit(livestock_data)
    
    
    
    
    
    
    
    
    
    
    # Finishing Touches --------------------------------------------
    
    # We have all the data we need, though there are still a couple
    # problems with our output.
    #   1. We want a date column
    #   2. We want to remove the "s" at the end of the 
    #      type column to account for cases when there is only one
    #      cow bought by the buyer (Ex. "black cow" and "black cows"
    #      should both read "black cow")
    
    # Adding in the date
    livestock_data <- mutate(livestock_data, "date" = date_of_sale, .before = 1)
    
    
    
    
    
    
    
    
    
    
    # Writing to CSV ---------------------------------------
    # We are now ready to write the data to a file!
    # Just remember to add in the column names if
    # this file is not already on your computer
    write_csv(x = livestock_data,
              file = "La Junta Market Reports.csv",
              append = T,
              col_names = F)
    
    
    
    
    # Confirmation message saying data was added to the file
    cat(paste0("\nDATA ADDED: ", date_of_sale, "\tURL: ", URL, "\n"))
    
  } # end of for loop
  
  
}#end of collection()

```

# Implementation

I have been changing the URLs around to find different data, and the market reports seem to start around ID == 6700. The max URL at the time of writing is around 13500. The data begins in 2016. In a few instances, the same market report may have two separate market IDs, so a few hundred observations could be replicas.

```{r urls-execute}

# This parses together different market report IDs and stores them in a vector
urls <- paste0("http://www.winterlivestock.com/lajunta.php?reportID=", 6680:13500, "#marketreport")

#the most recent market report
#urls <- "http://www.winterlivestock.com/lajunta.php"

# No need to save the output--this function writes to a csv file...
collection(urls = urls)

```

# Cleaning

The final step before we are ready for analysis is data cleaning. There are two primary goals in the cleaning script. First, we want to add column names to the csv. The second goal is to organize cattle into a smaller number of distinct groups.

Goal 1 is a piece of cake, but goal 2 is easier said than done. You can analyze some of my judgement calls and decide for yourself what needs to be done to group the different types of livestock.

Do you think that prices should be adjusted for inflation at some point in the future?

If you have trouble reading this csv file, click on Tools \> Global Options \> RMarkdown, then set 'Evaluate chunks in directory:' to Project.

A copy of the csv file will be created so we don't have to rerun the collection() function should the cleaning chunk need updating.

```{r cleaning}

# Step 0: save a copy of the file!
file.copy(from = "La Junta Market Reports.csv", to = "La Junta Market Reports (before cleaning).csv", overwrite = T)


# Step 1: adding a header to the csv --------------------------
lajunta <- readr::read_csv("La Junta Market Reports.csv", 
                           col_names = F)

# Adding in the column names
lajunta <- lajunta %>% 
  rename("Date" = X1,
         "Buyer" = X2,
         "Quantity" = X3,
         "Type" = X4,
         "Weight" = X5,
         "Price" = X6)

# Putting the entries in chronological order,
# first changing the data type to a Date:
lajunta$Date <- lubridate::mdy(lajunta$Date)












# Step 2: categorizing cattle ---------------------------------
# Removing the plural of the type (Ex. "black cows" becomes "black cow")
lajunta$Type <- str_remove(lajunta$Type, "s$")

# Making a column assigning cattle reproductive status
lajunta <- lajunta %>% 
  mutate(Reprod = str_extract(lajunta$Type, "hfr$|str$|bull$|cow$"))

# The bulk of the edits
lajunta$Type <- lajunta %>% 
  dplyr::select(Type) %>% 
  unlist() %>% 
  str_remove_all("\\.|,") %>%                     # removing punctuation
  str_remove_all("\\sx.*$|\\s[^\\s]*$") %>%  # removing " x hfr", " hfr", " x str", etc.
  str_replace_all("angus", "ang") %>% 
  str_remove_all("sim-") %>%                      # removing the sim- from sim-angus
  str_replace_all("sim\\s+ang", "ang") %>%        # removing "sim " from "sim angus"
  str_replace_all("\\s*sim", " ang") %>%          # replacing " sim" with " ang"
  str_replace_all("beefmaster|beefmstr", "bfmstr") %>%     # replacing "beefmaster" with "bfmstr"
  str_replace_all("shorthorn|shthrn", "sthrn") %>%# replacing "shorthorn" with "sthrn"
  str_replace_all("clr longhorn", "lnhrn") %>%    # replacing "longhorn" with "lnhrn" 
  str_replace_all("limousin", "lim") %>% 
  str_replace_all("limo", "lim") %>%              # replacing "limo" with "lim"
  str_replace_all(".*\\s+lim|lim flex", "lim") %>%# combining all different types of limousins 
  str_replace_all("gelbvieh|gelb|gelvieh", "gel") %>% 
  str_replace_all("hereford", "here") %>% 
  str_replace_all("charolais", "char") %>% 
  str_replace_all("brahman", "brah") %>%
  str_remove_all("wf-|bwf-|rwf-") %>%             # removing face types from the data when there is other info
  str_remove_all("\\s*&") %>%                     # removing ampersands
  str_replace_all("\\s?blk\\s?", "black") %>%     # replacing abbreviation "blk" with "black"
  str_replace_all("red black", "black red") %>%   # reordering "red and black" to "black and red"
  str_replace_all(".*-.*|mixed", "mix") %>%       # replaces any named cross-breed with "mix"
  str_replace_all("ang char|char ang", "mix") %>% # replaces "char ang" with "mix" explicitly 
  str_replace_all("balancer|bal|bclr", "mix") %>% # replaces "balancer" with "mix", since it's a hybrid
  str_replace_all("santa gert", "mix") %>%        # replaces "santa gert" with "mix", since it's a hybrid
  str_replace_all("stabilizer|stab", "mix") %>%   # replaces "stabilizer" with "mix", since it's a hybrid 
  str_replace_all("barzona", "mix") %>%           # replaces "barzona" with "mix", since it's a hybrid
  str_remove_all("\\s+wf|\\s+bwf|\\s+rwf") %>%    # removing face types at after the name
  str_replace_all("^wf$", "bwf") %>%              # recoding "wf" as "bwf" because it performs similarly and has few observations
  str_remove_all("wf\\s+|bwf\\s+|rwf\\s+") %>%    # removing face types before the name
  #str_replace_all("here|brah", "red") %>%         # replacing "here" and "brah" with "red" due to few observations and similar weight (Since the sample size increased, these cattle can now be kept)
  str_replace_all("black\\s+(ang)", "\\1") %>%    # removing color starts on this line
  str_replace_all("black\\s+(lim)", "\\1") %>% 
  str_replace_all("black\\s+(gel)", "\\1") %>% 
  str_replace_all("black\\s+(here)", "\\1") %>% 
  str_replace_all("black\\s+(char)", "\\1") %>% 
  str_replace_all("red\\s+(ang)", "\\1") %>% 
  str_replace_all("red\\s+(lim)", "\\1") %>% 
  str_replace_all("red\\s+(gel)", "\\1") %>% 
  str_replace_all("red\\s+(here)", "\\1") %>% 
  str_replace_all("red\\s+(char)", "\\1") %>%     # removing color ends on this line
  str_squish() %>% 
  str_trim()





#combining all colors into a new category--"clr" (optional)
lajunta$Type <- lajunta %>% 
  dplyr::select(Type) %>% 
  unlist() %>% 
  str_replace_all("black red|black|balck|red|gray|grey|roan|brown|brwn|bwn", "clr") %>%   # "black red," "black," and "red" are all colors
  str_replace_all("bwf|rwf|wf|spot", "face") %>%
  str_replace_all("face", "clr")                  # remove this line if you want to stop pooling the face group with the color group








# Miscellaneous Corrections -----------------------------------
# This correction of the Type column is done after finding errors explicitly. I am 'hard-coding' in the correct values. I checked the distinct Type values after running the above lines in this section, then fixed the remaining values to fit into a few categories. An admittedly lazy way of doing things, but the file is small--and I'm not a software engineer!
#Note: many of these operations assume that cross-breeds have already been assigned "mix"
lajunta$Type <- lajunta %>%
  dplyr::select(Type) %>%
  unlist() %>%
  str_remove_all("x ") %>%                  # removing stray 'x' characters
  str_replace_all("weaned", "clr") %>%      # Weaned cattle isn't a type, so it gets pooled in with "clr"
  str_replace_all("sal", "clr") %>%         # I don't know what type of cattle "sal" is, so it gets pooled in with "clr"
  str_replace_all("lh", "clr") %>%          # I don't know what type of cattle "lh" is, so it gets pooled in with "clr"
  str_replace_all("brnd", "clr") %>%        # I don't know what type of cattle "brnd" is, so it gets pooled in with "clr"
  str_replace_all("clrrod", "clr") %>%      # I don't know what type of cattle "clrrod" is (before "clr" was appended to the name in an earlier operation), so it gets pooled in with "clr"
  str_replace_all("mis", "clr") %>%         # I don't know what type of cattle "mis" is, so it gets pooled in with "clr"
  str_replace_all(".*char$", "char") %>%    # removing all characters preceding "char"
  str_replace_all("chr|char.*", "char") %>% # replacing "chr" with "char"
  str_replace_all(".*ang$", "ang") %>%      # removing all characters preceding "ang"
  str_replace_all("angmental", "ang") %>%   # replacing "angmental" with "ang"
  str_replace_all(".*maine$", "maine") %>%  # removing all characters preceding "maine"
  str_replace_all("maine", "me") %>%        # replacing "maine" with "me"
  str_remove_all("irish") %>%               # removing the word "irish" anywhere it appears (usually accompanying 'red')
  str_replace_all(".*clr$", "clr") %>%      # removing all characters preceding "clr" (this can only be done after the cases for more specific types [like char] have been addressed)
  str_replace_all("hererord", "here") %>%  # Fixing Errors
  str_replace_all("ang strs", "ang") %>% 
  str_replace_all("mixck", "mix")          # End of Fixing Errors


# After doing more testing it would appear that the remaining breeds do not have a large enough sample to be worth using. All cattle types not in the below list will be assigned to "clr":
types <- c("clr", "ang", "mix", "char", "lim", "gel", "here", "mot")

lajunta$Type[which(!lajunta$Type %in% types)] <- "clr"



# We now have eight categories in the "Type" variable








# Step 3: Writing to file -------------------------------------
write_csv(x = lajunta,
          file = "La Junta Market Reports.csv",
          append = F,
          col_names = T)

```

Of course, we want to make sure everything looks good, no? This section identifies problem areas in the file.

```{r testing}

lajunta <- read_csv("La Junta Market Reports.csv")


# finds counts of all types--good for finding how many types are in the data
count(lajunta, Type) %>% 
  arrange(desc(n)) %>% 
  as.data.frame()


# relevant info when imputing values:
lajunta %>% 
  group_by(Reprod) %>% 
  dplyr::summarize("Median price" = median(Price, na.rm = T),
            "Median weight" = median(Weight, na.rm = T),
            "Median quantity" = median(Quantity, na.rm = T),
            "Mean Price" = mean(Price, na.rm = T),
            "Mean weight" = mean(Weight, na.rm = T),
            "Mean quantity" = mean(Quantity))
# KNN is probably the best imputation method for this data


# You can see the distribution of missing Reprod values
lajunta %>% 
  filter(is.na(Reprod)) %>% 
  ggplot() +
  geom_col(mapping = aes(x = Weight, y = Price))

# see any patterns? Can you impute any values using this information (particularly on Reprod)?
lajunta %>% 
  select(Type, Weight, Reprod) %>% 
  filter(is.na(Reprod)) %>% 
  as.data.frame()

# How many sales were there on a given day?
lajunta %>% 
  count(Date)

# how many missing values are in the Date column?
lajunta %>% 
  filter(is.na(Date))

# Taking a look at the newest data
tail(lajunta)

```
