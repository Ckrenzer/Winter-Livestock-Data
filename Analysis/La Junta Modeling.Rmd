---
title: "La Junta Models"
author: "Connor Krenzer"
date: "2/7/2021"
output: html_document
---

The Shiny app will be self-contained. Rather than cluttering the Shiny app, I have decided to create this Rmarkdown document for further exploration into the data.

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      include = TRUE,
                      results = "hide",
                      error = F,
                      warning = F,
                      message = F)
```



# The Dataset
The La Junta dataset is quickly becoming one of my favorites. I suppose there is something to be said about collecting, cleaning, and analyzing data all by yourself.

```{r packages-import}
if(!require(pacman)) install.packages("pacman")

# General Packages
pacman::p_load(dplyr, readr, ggplot2,
               patchwork, lubridate,
               knitr, broom)

# KNN Packages
pacman::p_load(rsample, class)

# Logistic Regression Packages
pacman::p_load(stringr, pscl, caret,
               car, InformationValue)


# The raw data for La Junta, which is on GitHub
# (as opposed to the local file):
lajunta <- read_csv(paste0("https://raw.githubusercontent.com",
                      "/Ckrenzer/Winter-Livestock-Data/main",
                      "/La%20Junta%20Market%20Reports.csv"))

# Know that I only used paste0() on the url
# to avoid getting into that dark LaTex magic
# on pdf output!
```

To familiarize you with the dataset, let's take a quick peek:
```{r the dataset, echo = FALSE, results = "show"}
knitr::kable(lajunta %>% 
               select(-Buyer) %>% 
               slice(1:8))
```

You can see that we have data for the date of the sale, the quantity sold, the type of livestock, the weight in pounds, the price in USD, and the livestock's reproductive status. There is also data on Buyer names, but I thought it would be rude to include that information in this table. It hasn't proved very useful anyway.

From my previous work (see "[La Junta Predictions.Rmd](https://github.com/Ckrenzer/Winter-Livestock-Data/blob/main/Analysis/La%20Junta%20Predictions.Rmd)," for instance), I can confidently tell you that the "Type" column is utterly useless. The most telling graph I can provide for you is this one:
```{r graph, echo = FALSE, results = "show"}
lajunta %>% 
  ggplot() +
  geom_point(mapping = aes(x = Weight, y = Price, color = Reprod)) +
  ggtitle("Price vs. Weight") +
  theme_dark()

```

We can clearly see four distinct groups. We will explore these relationships in various ways.



# KNN Clustering
Since the data clusters so nicely, I could not resist checking KNN's performance!

### Step 1: Packages

I will be using the rsample package to split the data into testing and training sets. The class package will be used for the KNN algorithm. With this algorithm, I will be classifying cattle reproductive status (cow, bull, steer, or heifer) using weight and market price.




### Step 2: Prepare Data

Lucky for me, I already spent a full day cleaning this data! All I have to do is select relevant columns and make the character column a factor.

There are only two predictors in this model, weight and price.

```{r prepare}
# Fortunately, KNN is well-suited for just a few predictors
lajunta_knn <- lajunta %>% 
  select(Weight, Price, Reprod)

# storing the string vector as a factor
lajunta_knn$Reprod <- as.factor(lajunta_knn$Reprod)

```



### Step 3: Standardize Data

Since KNN is sensitive to different scales, so I will standardize the values of the predictors.

```{r standardize}
# The normalizing function, which transforms
# all values into the range from 0 to 1
normalize <- function(x) {
 return((x - min(x)) / (max(x) - min(x))) 
}


# Applying normalize() on the predictor
# columns
lajunta_knn <- lajunta_knn %>% 
  mutate(across(where(is.numeric), normalize))
```



### Step 4: Separate into Training and Test Sets

A stratified, simple random sampling technique will be used to separate values into training and testing data. Stratification is necessary in this instance because there are far fewer cows and bulls in the dataset than there are heifers or steers. Eighty percent of the values will be placed in the training set and the remaining twenty percent will go to the test set.

```{r splicing}
# Setting the random number stream
set.seed(2021)
lajunta_knn_split <- rsample::initial_split(lajunta_knn,
                               prob = 0.80,
                               strata = Reprod)

# As of 2/6/2021, the split is 869 in train
# and 289 in test, with a total of 1158 observations

# separating the data into two new data frames
lajunta_knn_train <- rsample::training(lajunta_knn_split)
lajunta_knn_test <- rsample::testing(lajunta_knn_split)
```



### Step 5: Build Model

To determine the appropriate number for K, I took the square root of the number of observations in the training set.

```{r the model}
# Determining the appropriate value of K
sqrt(nrow(lajunta_knn_train)) ## 29.47881

# Let's use K = 29

# the cl argument is the factor of the true
# classifications from the training set
knn_29 <- class::knn(train = lajunta_knn_train[1:2],
                     test = lajunta_knn_test[1:2],
                     cl = lajunta_knn_train$Reprod,
                     k = 29)


```



### Step 6: Evaluate

To calculate the accuracy, the data was thrown into a confusion matrix and the number of correct classifications were calculated as a rate. Though not shown, all cows and bulls are predicted perfectly. The only data which are not classified correctly are steers and heifers.

```{r evaluation}
# Create a confusion matrix--The result from knn()
# and the test data dependent variable are the
# arguments
knn_29_mat <- table(knn_29, lajunta_knn_test$Reprod)


#This function measures accuracy. It takes the confusion matrix as input
accuracy <- function(x){
  return(sum(diag(x)/(sum(rowSums(x)))) * 100)
  }


accuracy(knn_29_mat) # 87.2% as of 2/6/2021

```

### Step 7: Optimize

We can always do better, no? Let's run this again in a loop with changing values of K. The K with the greatest accuracy should be used for all future predictions. Also, this value should be somewhat close to 29--if we get a number very far from 29, it is likely that we are overfitting the model. That pesky bias-variance trade-off...

```{r optimization}
# Since I am using a loop, best practice
# is to pre-allocate space in the vector
acc <- numeric(100)
for(i in 1:100){
  knn_mod <- class::knn(train = lajunta_knn_train[1:2],
                       test = lajunta_knn_test[1:2],
                       cl = lajunta_knn_train$Reprod,
                       k = i)
  acc[i] <- accuracy(table(knn_mod, lajunta_knn_test$Reprod))
  
}

# finds the value in the vector named
# "acc" with the greatest accuracy
which(acc == max(acc)) ## 30  31

# As you can see, 31 was VERY close to our predicted 29 for a good value for K
acc[31] ## 87.54325
```

The ideal value was either 30 or 31--very close to 29! Using just two variables, we can predict the reproductive status of livestock with 87.5% accuracy! As previously stated, most of these discrepancies are between steers and heifers. If we are only interested in cows and bulls, we will virtually _always_ have the right prediction!







# Linear Regression
The data appears to follow a linear trend, so linear models seem very appropriate for this dataset. There is a linear model for each of the reproductive status. You can see the model's parameters below:


```{r linear models}
# All results are from 2/6/2021...


steer_fit <- lm(data = lajunta,
                subset = Reprod == "str",
                formula = Price ~ I(lubridate::mdy(Date)) +
                  Quantity + poly(Weight, degree = 2))


heifer_fit <- lm(data = lajunta,
                 subset = Reprod == "hfr",
                 formula = Price ~ I(lubridate::mdy(Date)) +
                   Quantity + poly(Weight, degree = 2))


# a 12th degree polynomial gives us an R-squared
# of .54, but I think that is due to overfitting
cow_fit <- lm(data = lajunta,
              subset = Reprod == "cow",
              formula = Price ~ I(lubridate::mdy(Date)) +
                poly(Weight, 3))


# A 6th degree polynomial gives an R-squared of .87,
# but I think that is due to overfitting...
# Our sample is small so we want to keep the
# number of predictors small
bull_fit <- lm(data = lajunta,
               subset = Reprod == "bull",
               formula = Price ~ I(lubridate::mdy(Date)) +
                 poly(Weight, 2))
```

Now, let's see how these models performed:

```{r lm evaluation, results = "show"}

knitr::kable(summary(steer_fit) %>% 
  broom::tidy(), caption = "Steer Model Evaluation")
# R-squared: 0.8933
# Adj. R-squared: 0.8926
# RSE: 7.199


knitr::kable(summary(heifer_fit) %>% 
  broom::tidy(), caption = "Heifer Model Evaluation")
# R-squared: 0.7313
# Adj. R-squared: 0.7289
# RSE: 6.516


knitr::kable(summary(cow_fit) %>% 
  broom::tidy(), caption = "Cow Model Evaluation")
# R-squared: 0.4206
# Adj. R-squared: 0.3884
# RSE: 3.037


knitr::kable(summary(bull_fit) %>% 
  broom::tidy(), caption = "Bull Model Evaluation")
# R-squared: 0.7564
# Adj. R-squared: 0.7232
# RSE: 2.431

```

Linear regression is an art as much as it is a science. I could write a [30 page paper](https://github.com/Ckrenzer/Nursing-Homes-and-COVID/raw/main/Improving%20Econometrics/Nursing%20Homes%20and%20COVID.docx) explaining all the EDA, providing a play-by-play commentary about potential improvements to this model. But today I think I'll leave things as they are presented here.

See the accompanying Shiny app (linked above) if you'd like to play with these linear models yourself!











# Logistic Regression

There are a few things I want to try with a logistic regression. Though logistic regression can handle more than two categories, it is best-suited for binary dependent variables. In these regressions, I will be evaluating three models with binary outcomes:

1. Determining whether livestock is a steer or a heifer.
2. Determining whether livestock is a bull or a cow.
3. Determining whether livestock is either a bull/cow or steer/heifer.

Due to enormous differences between cows/bulls and heifers/steers, I expect zero error in the third regression.

### Setup and Cleaning

First things first. In the first regression, only data for steers and heifers will be included. In the second regression, only data for bulls and cows will be included. In the third regression, steers and heifers will be recoded as "y," for "young," while cows and bulls will be recoded as "par," for "parents" (and yes, I understand that bulls are not necessarily fathers; I had trouble coming up with a name for these categories). With this in mind, let's store data for the models:

```{r data subsets}
# Data for regression 1
lajunta_logit1 <- lajunta %>% 
  filter(Reprod %in% c("str", "hfr")) %>% 
  mutate(across(where(is.character), as.factor))


# Data for regression 2
lajunta_logit2 <- lajunta %>% 
  filter(Reprod %in% c("bull", "cow")) %>% 
  mutate(across(where(is.character), as.factor))

# Data for regression 3
lajunta_logit3 <- lajunta %>% 
  mutate(Reprod = lajunta[["Reprod"]] %>% 
           str_replace_all("bull|cow", "par") %>% 
           str_replace_all("str|hfr", "y")) %>% 
  mutate(across(where(is.character), as.factor))

```

Now that we have everything in a tibble, we are ready to split up the data.




### Split into training

The data will be split into testing and training sets with an 80/20 split. For the same reasons used in the KNN classification section, stratified sampling will be used.

```{r data splicing}

# REGRESSION 1 (steer vs heifer)
# Setting the random number stream
set.seed(2021)
# Stratification for the first regression
lajunta_logit1_split <- rsample::initial_split(lajunta_logit1,
                               prob = 0.80,
                               strata = Reprod)
# separating the data into two new data frames
lajunta_logit1_train <- rsample::training(lajunta_logit1_split)
lajunta_logit1_test <- rsample::testing(lajunta_logit1_split)



# REGRESSION 2 (bull vs cow)
# Setting the random number stream again
# (in the event R tries to pull some
# funny business)
set.seed(2021)
# Stratification for the first regression
lajunta_logit2_split <- rsample::initial_split(lajunta_logit2,
                               prob = 0.80,
                               strata = Reprod)
# separating the data into two new data frames
lajunta_logit2_train <- rsample::training(lajunta_logit2_split)
lajunta_logit2_test <- rsample::testing(lajunta_logit2_split)





# REGRESSION 3 (bull/cow vs. steer/heifer)
# Setting the random number stream again
# (in the event R tries to pull some
# funny business)
set.seed(2021)
# Stratification for the first regression
lajunta_logit3_split <- rsample::initial_split(lajunta_logit3,
                               prob = 0.80,
                               strata = Reprod)
# separating the data into two new data frames
lajunta_logit3_train <- rsample::training(lajunta_logit3_split)
lajunta_logit3_test <- rsample::testing(lajunta_logit3_split)
```


### Models

The data is now ready for model assembly. I will be using the stats package's glm() function as the engine for the regressions.

```{r logistic model}
# REGRESSION 1 (steer vs heifer)
# Quantity is not very different between steers and heifers
logit1 <- glm(data = lajunta_logit1_train,
              formula = Reprod ~ Weight + Price,
              family = "binomial")

# REGRESSION 2 (bull vs cow)
logit2 <- glm(data = lajunta_logit2_train,
              formula = Reprod ~ Weight + Price + Quantity,
              family = "binomial")

# REGRESSSION 3 (bull/cow vs. steer/heifer)
logit3 <- glm(data = lajunta_logit3_train,
              formula = Reprod ~ Weight + Price + Quantity,
              family = "binomial")
```


Here are the results from the first regression (steers vs heifers):
```{r summary 1, echo = FALSE, results = "show"}

summary(logit1)

```

From the second regression (bull vs cow):
```{r summary 2, echo = FALSE, results = "show"}

summary(logit2)

```

And the third regression (bull/cow vs. steer/heifer):
```{r summary 3, echo = FALSE, results = "show"}

summary(logit3)

```

The predictors in the first regression did a pretty good job. Judging by the p-values, the predictors in the second and third regressions did an abysmal job!

The correct interpretation of the Price variable in the first regression, however, is that a one dollar increase in the price of the livestock is associated with an average increase of 0.207 in the log odds of being a steer.

Well, this is quite strange. Let's continue on to the evaluation and see if we can figure out what's going on.


### Evaluation

#### McFadden's R-square
We can use the pR2() function from the pscl package to compute McFadden's R-square value, which ranges from 0 to just under 1. Values close to 0 indicate the model as no predictive power. Values over 0.4 suggest the model is a good fit for the data.


Let's begin with the first regression:
```{r McFadden R-squared (logit1), echo = FALSE, results = "show"}
# REGRESSION 1 (steer vs heifer)
pscl::pR2(logit1)["McFadden"]

```

As expected, the predictors in the first regression do a pretty good job. Let's see the McFadden statistic from the second regression:
```{r McFadden R-squared (logit2), echo = FALSE, results = "show"}
# REGRESSION 2 (bull vs cow)
pscl::pR2(logit2)["McFadden"]

```


Ah! I see now. The data fits these points so well that the model is deterministic! I expect this to be the case for the third regression as well:
```{r McFadden R-squared (logit3), echo = FALSE, results = "show"}
# REGRESSSION 3 (bull/cow vs. steer/heifer)
pscl::pR2(logit3)["McFadden"]

```

Yup.


#### Variable Importance
We can compute the importance of each predictor variable in the model by using the varImp() function from the caret package. Higher values indicate more importance. These values can help confirm the validity of p-values.
```{r variable importance}
caret::varImp(model)


```

#### Variance Inflation Factor
We can calculate the VIF of each variable to see if multicollinearity is a problem. Since there are so few variables, multicollinearity is unlikely to have much effect on the results. VIF scores greater than 5 indicate multicollinearity is an issue that needs to be addressed.

```{r VIF}
#calculate VIF values for each predictor variable in our model
car::vif(model)


```


#### Test Data
Finally, we can evaluate model performance by using the test dataset. By default, any observation in the test dataset with a probability greater than 0.5 will be predicted to be a parent. We can calculate these probabilities with this code:

```{r probabilities}
predict(model, test, type="response")

```


Thankfully, mathematical wizards around the world have put together a magical way of determining the optimal probability to maximize the probability of the model. The optimalCutoff() function from the InformationValue package does exactly that.

```{r evaluation}
#convert defaults from "Yes" and "No" to 1's and 0's
test$default <- ifelse(test$default=="Yes", 1, 0)

#find optimal cutoff probability to use to maximize accuracy
optimal <- InformationValue::optimalCutoff(test$default, predicted)[1]
optimal

##[1] 0.5451712
```

Using the optimal threshold, we can create a confusion matrix showing how the model performed on the test data.

```{r confusion matrix}
InformationValue::confusionMatrix(test$default, predicted)

```
 
 We can also calculate the sensitivity (the "true positive rate"), specificity (the "true negative rate"), and the total number of misclassifications.


```{r}
#calculate sensitivity
InformationValue::sensitivity(test$default, predicted)

##[1] 0.3786408

#calculate specificity
InformationValue::specificity(test$default, predicted)

##[1] 0.9928401

#calculate total misclassification error rate
InformationValue::misClassError(test$default, predicted, threshold=optimal)

##[1] 0.027

# total misclassification error rate is 2.7%

```



#### ROC Curve

Finally, we can put together ROC curves to visualize how well the models fit. The greater the AUC (area under the curve), the more accurately the model predicts outcomes.


```{r roc curve}
#plot the ROC curve
InformationValue::plotROC(test$default, predicted)


# be sure to use the dark theme...

```








# Linear Discrminant Analysis